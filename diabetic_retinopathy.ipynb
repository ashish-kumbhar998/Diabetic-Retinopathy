{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2OFX9YUGHKAd"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from numpy import genfromtxt\n","from tensorflow.keras.preprocessing import image\n","\n","from tensorflow.keras.utils import img_to_array\n","from tensorflow.keras.utils import array_to_img\n","from tensorflow.keras.applications.imagenet_utils import preprocess_input\n","from keras.utils import np_utils\n","from sklearn.model_selection import train_test_split\n","from keras.applications import ResNet50\n","from keras.models import Model, Sequential\n","from keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input\n","from keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from keras.applications import ResNet50                                   ### Import resnet50 as our pre-trained model for CNN\n","from keras.layers import BatchNormalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JxCxuPgpwaD3"},"outputs":[],"source":["trainLabelscsv = genfromtxt('/content/drive/MyDrive/Diabetic/trainDR2.csv',dtype=str, delimiter=',')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1677491630969,"user":{"displayName":"Ashish Kumbhar","userId":"04417448536066889477"},"user_tz":-330},"id":"wW_H56YYo1TY","outputId":"a614ab99-56a5-40a7-c342-c3ff9755935d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3663"]},"metadata":{},"execution_count":4}],"source":["trainLabelscsv.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugnNKmElZvEt"},"outputs":[],"source":["PATH='/content/drive/MyDrive/Diabetic/DR2/'\n","files=os.listdir(PATH)\n","img_data_list=[]\n","labels=[]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vHRQCAOZaQf8"},"outputs":[],"source":["def preprocessImageData():\n","    y=trainLabelscsv.shape[0]\n","    for i in files:\n","        try:\n","            img_path = PATH + i\n","            img = image.load_img(img_path, target_size=(224, 224))\n","            x = img_to_array(img)\n","            x = np.expand_dims(x, axis=0)\n","            x = preprocess_input(x)\n","            # print('Input image shape:', x.shape)\n","            img_data_list.append(x)\n","            for j in range(0,y):\n","                if i==trainLabelscsv[j][0]+'.png':\n","                        # print(trainLabelscsv[j][1])\n","                        labels.append(trainLabelscsv[j][1])\n","        except:   \n","            pass   "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31752,"status":"ok","timestamp":1677491662690,"user":{"displayName":"Ashish Kumbhar","userId":"04417448536066889477"},"user_tz":-330},"id":"rVH0K5tV4fRC","outputId":"2a54708a-2500-483c-d5a7-6594897df612"},"outputs":[{"output_type":"stream","name":"stdout","text":["(3662, 1, 224, 224, 3)\n"]}],"source":["preprocessImageData()\n","X = np.array(img_data_list)\n","print(X.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1677491662691,"user":{"displayName":"Ashish Kumbhar","userId":"04417448536066889477"},"user_tz":-330},"id":"fojdjDAXtqad","outputId":"bca84515-ebf7-4dfd-fa72-0988bde315a2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{},"execution_count":8}],"source":["type(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqICw3jb0TwL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677491662691,"user_tz":-330,"elapsed":45,"user":{"displayName":"Ashish Kumbhar","userId":"04417448536066889477"}},"outputId":"46c538b6-670d-41d6-c4eb-adc3235d021d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[[[[-101.939 -116.779 -121.68 ]\n","    [-101.939 -116.779 -121.68 ]\n","    [-101.939 -116.779 -121.68 ]\n","    ...\n","    [-101.939 -116.779 -121.68 ]\n","    [-102.939 -116.779 -122.68 ]\n","    [-102.939 -116.779 -122.68 ]]\n","\n","   [[-101.939 -116.779 -121.68 ]\n","    [-101.939 -116.779 -121.68 ]\n","    [-101.939 -116.779 -121.68 ]\n","    ...\n","    [-102.939 -116.779 -122.68 ]\n","    [-102.939 -116.779 -122.68 ]\n","    [-101.939 -116.779 -121.68 ]]\n","\n","   [[-101.939 -116.779 -121.68 ]\n","    [-101.939 -116.779 -121.68 ]\n","    [-101.939 -116.779 -121.68 ]\n","    ...\n","    [-102.939 -116.779 -122.68 ]\n","    [-102.939 -116.779 -122.68 ]\n","    [-101.939 -116.779 -121.68 ]]\n","\n","   ...\n","\n","   [[-101.939 -116.779 -121.68 ]\n","    [-101.939 -116.779 -121.68 ]\n","    [-101.939 -116.779 -121.68 ]\n","    ...\n","    [-101.939 -116.779 -121.68 ]\n","    [-102.939 -116.779 -122.68 ]\n","    [-101.939 -116.779 -121.68 ]]\n","\n","   [[-101.939 -116.779 -121.68 ]\n","    [-101.939 -116.779 -121.68 ]\n","    [-101.939 -116.779 -121.68 ]\n","    ...\n","    [-101.939 -116.779 -121.68 ]\n","    [-102.939 -116.779 -122.68 ]\n","    [-101.939 -116.779 -121.68 ]]\n","\n","   [[-102.939 -116.779 -122.68 ]\n","    [-101.939 -116.779 -121.68 ]\n","    [-102.939 -116.779 -122.68 ]\n","    ...\n","    [-101.939 -116.779 -121.68 ]\n","    [-101.939 -116.779 -121.68 ]\n","    [-101.939 -116.779 -121.68 ]]]\n","\n","\n","  [[[ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    ...\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]]\n","\n","   [[ -97.939 -112.779 -119.68 ]\n","    [ -96.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    ...\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]]\n","\n","   [[ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    ...\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -96.939 -111.779 -119.68 ]]\n","\n","   ...\n","\n","   [[ -97.939 -112.779 -119.68 ]\n","    [ -96.939 -112.779 -119.68 ]\n","    [ -96.939 -112.779 -119.68 ]\n","    ...\n","    [ -96.939 -112.779 -119.68 ]\n","    [ -96.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]]\n","\n","   [[ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -96.939 -112.779 -119.68 ]\n","    ...\n","    [ -96.939 -112.779 -119.68 ]\n","    [ -96.939 -113.779 -119.68 ]\n","    [ -96.939 -112.779 -119.68 ]]\n","\n","   [[ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -96.939 -112.779 -119.68 ]\n","    ...\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -96.939 -113.779 -118.68 ]\n","    [ -96.939 -112.779 -119.68 ]]]\n","\n","\n","  [[[-103.939 -115.779 -121.68 ]\n","    [-103.939 -115.779 -121.68 ]\n","    [-103.939 -114.779 -120.68 ]\n","    ...\n","    [-102.939 -115.779 -121.68 ]\n","    [-101.939 -115.779 -120.68 ]\n","    [-101.939 -115.779 -120.68 ]]\n","\n","   [[-103.939 -115.779 -121.68 ]\n","    [-103.939 -115.779 -121.68 ]\n","    [-103.939 -114.779 -120.68 ]\n","    ...\n","    [-102.939 -115.779 -122.68 ]\n","    [-102.939 -116.779 -121.68 ]\n","    [-102.939 -116.779 -121.68 ]]\n","\n","   [[-103.939 -115.779 -121.68 ]\n","    [-103.939 -115.779 -121.68 ]\n","    [-102.939 -114.779 -120.68 ]\n","    ...\n","    [-102.939 -115.779 -122.68 ]\n","    [-102.939 -116.779 -121.68 ]\n","    [-102.939 -116.779 -121.68 ]]\n","\n","   ...\n","\n","   [[-101.939 -114.779 -121.68 ]\n","    [-103.939 -116.779 -123.68 ]\n","    [-103.939 -116.779 -123.68 ]\n","    ...\n","    [-103.939 -116.779 -123.68 ]\n","    [-103.939 -116.779 -123.68 ]\n","    [-103.939 -116.779 -123.68 ]]\n","\n","   [[-103.939 -116.779 -123.68 ]\n","    [-103.939 -116.779 -123.68 ]\n","    [-103.939 -116.779 -123.68 ]\n","    ...\n","    [-103.939 -116.779 -123.68 ]\n","    [-103.939 -116.779 -123.68 ]\n","    [-103.939 -116.779 -123.68 ]]\n","\n","   [[-103.939 -116.779 -123.68 ]\n","    [-103.939 -116.779 -123.68 ]\n","    [-103.939 -116.779 -123.68 ]\n","    ...\n","    [-103.939 -116.779 -123.68 ]\n","    [-103.939 -116.779 -123.68 ]\n","    [-103.939 -116.779 -123.68 ]]]\n","\n","\n","  ...\n","\n","\n","  [[[ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    ...\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]]\n","\n","   [[ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    ...\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -113.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]]\n","\n","   [[ -97.939 -113.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    ...\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -113.779 -119.68 ]]\n","\n","   ...\n","\n","   [[ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -113.779 -119.68 ]\n","    ...\n","    [ -96.939 -112.779 -119.68 ]\n","    [ -96.939 -112.779 -119.68 ]\n","    [ -96.939 -112.779 -119.68 ]]\n","\n","   [[ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -98.939 -112.779 -119.68 ]\n","    ...\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -96.939 -112.779 -119.68 ]\n","    [ -97.939 -113.779 -119.68 ]]\n","\n","   [[ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    ...\n","    [ -96.939 -112.779 -119.68 ]\n","    [ -97.939 -112.779 -119.68 ]\n","    [ -97.939 -113.779 -119.68 ]]]\n","\n","\n","  [[[-101.939 -115.779 -120.68 ]\n","    [-101.939 -115.779 -120.68 ]\n","    [-102.939 -116.779 -121.68 ]\n","    ...\n","    [-101.939 -114.779 -121.68 ]\n","    [-101.939 -115.779 -120.68 ]\n","    [-101.939 -115.779 -120.68 ]]\n","\n","   [[-101.939 -115.779 -120.68 ]\n","    [-101.939 -115.779 -120.68 ]\n","    [-101.939 -115.779 -120.68 ]\n","    ...\n","    [-101.939 -114.779 -121.68 ]\n","    [-101.939 -115.779 -120.68 ]\n","    [-101.939 -115.779 -120.68 ]]\n","\n","   [[-101.939 -115.779 -120.68 ]\n","    [-101.939 -115.779 -120.68 ]\n","    [-101.939 -115.779 -120.68 ]\n","    ...\n","    [-101.939 -114.779 -121.68 ]\n","    [-101.939 -115.779 -120.68 ]\n","    [-101.939 -115.779 -120.68 ]]\n","\n","   ...\n","\n","   [[-101.939 -115.779 -120.68 ]\n","    [-101.939 -115.779 -120.68 ]\n","    [-101.939 -114.779 -121.68 ]\n","    ...\n","    [-101.939 -114.779 -120.68 ]\n","    [-101.939 -114.779 -121.68 ]\n","    [-101.939 -114.779 -120.68 ]]\n","\n","   [[-101.939 -115.779 -120.68 ]\n","    [-101.939 -115.779 -120.68 ]\n","    [-101.939 -114.779 -121.68 ]\n","    ...\n","    [-102.939 -115.779 -122.68 ]\n","    [-101.939 -114.779 -121.68 ]\n","    [-102.939 -115.779 -122.68 ]]\n","\n","   [[-101.939 -114.779 -121.68 ]\n","    [-101.939 -114.779 -121.68 ]\n","    [-101.939 -114.779 -121.68 ]\n","    ...\n","    [-101.939 -115.779 -120.68 ]\n","    [-101.939 -114.779 -121.68 ]\n","    [-102.939 -115.779 -122.68 ]]]\n","\n","\n","  [[[ -96.939 -110.779 -117.68 ]\n","    [ -96.939 -110.779 -117.68 ]\n","    [ -96.939 -110.779 -117.68 ]\n","    ...\n","    [ -96.939 -111.779 -117.68 ]\n","    [ -96.939 -111.779 -117.68 ]\n","    [ -96.939 -110.779 -117.68 ]]\n","\n","   [[ -96.939 -110.779 -117.68 ]\n","    [ -96.939 -110.779 -117.68 ]\n","    [ -96.939 -110.779 -117.68 ]\n","    ...\n","    [ -96.939 -111.779 -117.68 ]\n","    [ -96.939 -110.779 -117.68 ]\n","    [ -96.939 -111.779 -117.68 ]]\n","\n","   [[ -96.939 -110.779 -117.68 ]\n","    [ -97.939 -110.779 -117.68 ]\n","    [ -97.939 -110.779 -117.68 ]\n","    ...\n","    [ -96.939 -111.779 -117.68 ]\n","    [ -97.939 -110.779 -117.68 ]\n","    [ -96.939 -110.779 -117.68 ]]\n","\n","   ...\n","\n","   [[ -96.939 -110.779 -117.68 ]\n","    [ -96.939 -110.779 -117.68 ]\n","    [ -96.939 -110.779 -117.68 ]\n","    ...\n","    [ -96.939 -110.779 -117.68 ]\n","    [ -96.939 -110.779 -117.68 ]\n","    [ -96.939 -111.779 -117.68 ]]\n","\n","   [[ -96.939 -111.779 -117.68 ]\n","    [ -96.939 -110.779 -117.68 ]\n","    [ -96.939 -110.779 -117.68 ]\n","    ...\n","    [ -96.939 -110.779 -117.68 ]\n","    [ -96.939 -111.779 -117.68 ]\n","    [ -96.939 -111.779 -117.68 ]]\n","\n","   [[ -96.939 -111.779 -117.68 ]\n","    [ -96.939 -111.779 -117.68 ]\n","    [ -96.939 -111.779 -117.68 ]\n","    ...\n","    [ -96.939 -110.779 -117.68 ]\n","    [ -97.939 -111.779 -117.68 ]\n","    [ -96.939 -111.779 -117.68 ]]]]]\n"]}],"source":["X=np.rollaxis(X,1,0)\n","print(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ZVGumrNuyab","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677491662692,"user_tz":-330,"elapsed":42,"user":{"displayName":"Ashish Kumbhar","userId":"04417448536066889477"}},"outputId":"a9bea1d0-e032-4738-e0c4-9ad562c08675"},"outputs":[{"output_type":"stream","name":"stdout","text":["(3662, 224, 224, 3)\n"]}],"source":["X=X[0]\n","print (X.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MzHNSf3Xu_ht"},"outputs":[],"source":["num_of_samples = X.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1677491662693,"user":{"displayName":"Ashish Kumbhar","userId":"04417448536066889477"},"user_tz":-330},"id":"GMmY1liwwhG3","outputId":"854a2d95-61b1-4154-f650-e71cb471ede9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3662"]},"metadata":{},"execution_count":12}],"source":["num_of_samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0TVavrG1qze6"},"outputs":[],"source":["# labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9lzg5H2wnab"},"outputs":[],"source":["num_classes=5\n","Y = np_utils.to_categorical(labels, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1677491662695,"user":{"displayName":"Ashish Kumbhar","userId":"04417448536066889477"},"user_tz":-330},"id":"6jghsT06wvAK","outputId":"3926a587-0178-46ff-d09d-b53a2972a673"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{},"execution_count":15}],"source":["type(Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBdV20aash68"},"outputs":[],"source":["X = X.astype('float32')/255  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pg8su-zStxUN"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=40)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTzEXGHn0RIV"},"outputs":[],"source":["X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=40)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnLCfoXRtyvo"},"outputs":[],"source":["model = Sequential()\n","\n","model.add(ResNet50(include_top=False, pooling='avg', weights='imagenet'))\n","model.add(Flatten())\n","model.add(BatchNormalization())\n","model.add(Dense(2048, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dense(1024, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dense(512, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","model.layers[0].trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1677491670127,"user":{"displayName":"Ashish Kumbhar","userId":"04417448536066889477"},"user_tz":-330},"id":"Kd7ijyyixECg","outputId":"a325723e-53b9-4027-8a50-bd28a92082e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," resnet50 (Functional)       (None, 2048)              23587712  \n","                                                                 \n"," flatten (Flatten)           (None, 2048)              0         \n","                                                                 \n"," batch_normalization (BatchN  (None, 2048)             8192      \n"," ormalization)                                                   \n","                                                                 \n"," dense (Dense)               (None, 2048)              4196352   \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 2048)             8192      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_1 (Dense)             (None, 1024)              2098176   \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 1024)             4096      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_2 (Dense)             (None, 5)                 5125      \n","                                                                 \n","=================================================================\n","Total params: 29,907,845\n","Trainable params: 6,309,893\n","Non-trainable params: 23,597,952\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677491670128,"user":{"displayName":"Ashish Kumbhar","userId":"04417448536066889477"},"user_tz":-330},"id":"0liQG4nQyYnA","outputId":"80180ce3-3c25-43d7-8ee5-63c6e8c21226"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super().__init__(name, **kwargs)\n"]}],"source":["lr = 0.001\n","epochs = 100\n","bs = 20\n","optimizer = Adam(lr = lr, decay= lr/epochs)\n","model.compile(optimizer, loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kH9pdDmNy-Gc"},"outputs":[],"source":["train_datagen = ImageDataGenerator(rotation_range=15,fill_mode =\"nearest\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hadGJ4RvzjkA"},"outputs":[],"source":["checkpointer = ModelCheckpoint(filepath = \"/content/drive/MyDrive/Diabetic/model.h5\", save_best_only = True, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3888888,"status":"ok","timestamp":1677495559009,"user":{"displayName":"Ashish Kumbhar","userId":"04417448536066889477"},"user_tz":-330},"id":"rEa2Bk8K0o6s","outputId":"c7cd4295-f54a-41fa-c737-c4efe7ed5732"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","146/146 [==============================] - ETA: 0s - loss: 0.4408 - accuracy: 0.6525\n","Epoch 1: val_loss improved from inf to 0.40533, saving model to /content/drive/MyDrive/Diabetic/model.h5\n","146/146 [==============================] - 59s 331ms/step - loss: 0.4408 - accuracy: 0.6525 - val_loss: 0.4053 - val_accuracy: 0.5000\n","Epoch 2/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.6961\n","Epoch 2: val_loss improved from 0.40533 to 0.39096, saving model to /content/drive/MyDrive/Diabetic/model.h5\n","146/146 [==============================] - 41s 279ms/step - loss: 0.2874 - accuracy: 0.6961 - val_loss: 0.3910 - val_accuracy: 0.4727\n","Epoch 3/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.6937\n","Epoch 3: val_loss improved from 0.39096 to 0.34594, saving model to /content/drive/MyDrive/Diabetic/model.h5\n","146/146 [==============================] - 41s 278ms/step - loss: 0.2779 - accuracy: 0.6937 - val_loss: 0.3459 - val_accuracy: 0.6038\n","Epoch 4/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.7133\n","Epoch 4: val_loss improved from 0.34594 to 0.25055, saving model to /content/drive/MyDrive/Diabetic/model.h5\n","146/146 [==============================] - 40s 273ms/step - loss: 0.2653 - accuracy: 0.7133 - val_loss: 0.2505 - val_accuracy: 0.7322\n","Epoch 5/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.7102\n","Epoch 5: val_loss improved from 0.25055 to 0.24279, saving model to /content/drive/MyDrive/Diabetic/model.h5\n","146/146 [==============================] - 40s 277ms/step - loss: 0.2628 - accuracy: 0.7102 - val_loss: 0.2428 - val_accuracy: 0.7596\n","Epoch 6/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2646 - accuracy: 0.7109\n","Epoch 6: val_loss did not improve from 0.24279\n","146/146 [==============================] - 38s 262ms/step - loss: 0.2646 - accuracy: 0.7109 - val_loss: 0.2503 - val_accuracy: 0.7514\n","Epoch 7/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2546 - accuracy: 0.7171\n","Epoch 7: val_loss did not improve from 0.24279\n","146/146 [==============================] - 35s 240ms/step - loss: 0.2546 - accuracy: 0.7171 - val_loss: 0.2902 - val_accuracy: 0.7213\n","Epoch 8/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2563 - accuracy: 0.7264\n","Epoch 8: val_loss did not improve from 0.24279\n","146/146 [==============================] - 37s 250ms/step - loss: 0.2563 - accuracy: 0.7264 - val_loss: 0.2590 - val_accuracy: 0.7623\n","Epoch 9/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.7315\n","Epoch 9: val_loss did not improve from 0.24279\n","146/146 [==============================] - 36s 249ms/step - loss: 0.2484 - accuracy: 0.7315 - val_loss: 0.2566 - val_accuracy: 0.7295\n","Epoch 10/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2446 - accuracy: 0.7319\n","Epoch 10: val_loss did not improve from 0.24279\n","146/146 [==============================] - 35s 240ms/step - loss: 0.2446 - accuracy: 0.7319 - val_loss: 0.2562 - val_accuracy: 0.7268\n","Epoch 11/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.7332\n","Epoch 11: val_loss did not improve from 0.24279\n","146/146 [==============================] - 37s 250ms/step - loss: 0.2451 - accuracy: 0.7332 - val_loss: 0.2801 - val_accuracy: 0.7404\n","Epoch 12/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 0.7415\n","Epoch 12: val_loss did not improve from 0.24279\n","146/146 [==============================] - 35s 240ms/step - loss: 0.2392 - accuracy: 0.7415 - val_loss: 0.2528 - val_accuracy: 0.7432\n","Epoch 13/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2457 - accuracy: 0.7257\n","Epoch 13: val_loss did not improve from 0.24279\n","146/146 [==============================] - 38s 256ms/step - loss: 0.2457 - accuracy: 0.7257 - val_loss: 0.2834 - val_accuracy: 0.7240\n","Epoch 14/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.7329\n","Epoch 14: val_loss did not improve from 0.24279\n","146/146 [==============================] - 36s 247ms/step - loss: 0.2381 - accuracy: 0.7329 - val_loss: 0.2624 - val_accuracy: 0.7432\n","Epoch 15/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.7391\n","Epoch 15: val_loss improved from 0.24279 to 0.24123, saving model to /content/drive/MyDrive/Diabetic/model.h5\n","146/146 [==============================] - 39s 267ms/step - loss: 0.2381 - accuracy: 0.7391 - val_loss: 0.2412 - val_accuracy: 0.7541\n","Epoch 16/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2294 - accuracy: 0.7487\n","Epoch 16: val_loss did not improve from 0.24123\n","146/146 [==============================] - 37s 255ms/step - loss: 0.2294 - accuracy: 0.7487 - val_loss: 0.2510 - val_accuracy: 0.7514\n","Epoch 17/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2337 - accuracy: 0.7432\n","Epoch 17: val_loss did not improve from 0.24123\n","146/146 [==============================] - 35s 241ms/step - loss: 0.2337 - accuracy: 0.7432 - val_loss: 0.2518 - val_accuracy: 0.7514\n","Epoch 18/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.7480\n","Epoch 18: val_loss did not improve from 0.24123\n","146/146 [==============================] - 36s 250ms/step - loss: 0.2321 - accuracy: 0.7480 - val_loss: 0.2618 - val_accuracy: 0.7541\n","Epoch 19/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2382 - accuracy: 0.7384\n","Epoch 19: val_loss improved from 0.24123 to 0.23100, saving model to /content/drive/MyDrive/Diabetic/model.h5\n","146/146 [==============================] - 41s 284ms/step - loss: 0.2382 - accuracy: 0.7384 - val_loss: 0.2310 - val_accuracy: 0.7678\n","Epoch 20/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2233 - accuracy: 0.7535\n","Epoch 20: val_loss did not improve from 0.23100\n","146/146 [==============================] - 36s 243ms/step - loss: 0.2233 - accuracy: 0.7535 - val_loss: 0.2608 - val_accuracy: 0.7377\n","Epoch 21/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2325 - accuracy: 0.7456\n","Epoch 21: val_loss did not improve from 0.23100\n","146/146 [==============================] - 36s 248ms/step - loss: 0.2325 - accuracy: 0.7456 - val_loss: 0.2527 - val_accuracy: 0.7295\n","Epoch 22/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2292 - accuracy: 0.7425\n","Epoch 22: val_loss did not improve from 0.23100\n","146/146 [==============================] - 36s 248ms/step - loss: 0.2292 - accuracy: 0.7425 - val_loss: 0.2416 - val_accuracy: 0.7678\n","Epoch 23/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.7614\n","Epoch 23: val_loss did not improve from 0.23100\n","146/146 [==============================] - 35s 241ms/step - loss: 0.2218 - accuracy: 0.7614 - val_loss: 0.2461 - val_accuracy: 0.7486\n","Epoch 24/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2233 - accuracy: 0.7607\n","Epoch 24: val_loss did not improve from 0.23100\n","146/146 [==============================] - 37s 250ms/step - loss: 0.2233 - accuracy: 0.7607 - val_loss: 0.2426 - val_accuracy: 0.7486\n","Epoch 25/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2225 - accuracy: 0.7583\n","Epoch 25: val_loss did not improve from 0.23100\n","146/146 [==============================] - 35s 240ms/step - loss: 0.2225 - accuracy: 0.7583 - val_loss: 0.2375 - val_accuracy: 0.7377\n","Epoch 26/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.7528\n","Epoch 26: val_loss did not improve from 0.23100\n","146/146 [==============================] - 36s 247ms/step - loss: 0.2288 - accuracy: 0.7528 - val_loss: 0.2335 - val_accuracy: 0.7760\n","Epoch 27/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2261 - accuracy: 0.7542\n","Epoch 27: val_loss did not improve from 0.23100\n","146/146 [==============================] - 36s 248ms/step - loss: 0.2261 - accuracy: 0.7542 - val_loss: 0.2546 - val_accuracy: 0.7568\n","Epoch 28/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.7631\n","Epoch 28: val_loss did not improve from 0.23100\n","146/146 [==============================] - 35s 238ms/step - loss: 0.2173 - accuracy: 0.7631 - val_loss: 0.2798 - val_accuracy: 0.7377\n","Epoch 29/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2192 - accuracy: 0.7576\n","Epoch 29: val_loss did not improve from 0.23100\n","146/146 [==============================] - 35s 239ms/step - loss: 0.2192 - accuracy: 0.7576 - val_loss: 0.2642 - val_accuracy: 0.7240\n","Epoch 30/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.7683\n","Epoch 30: val_loss did not improve from 0.23100\n","146/146 [==============================] - 38s 257ms/step - loss: 0.2174 - accuracy: 0.7683 - val_loss: 0.2812 - val_accuracy: 0.7623\n","Epoch 31/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2192 - accuracy: 0.7576\n","Epoch 31: val_loss did not improve from 0.23100\n","146/146 [==============================] - 35s 237ms/step - loss: 0.2192 - accuracy: 0.7576 - val_loss: 0.2496 - val_accuracy: 0.7623\n","Epoch 32/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2131 - accuracy: 0.7625\n","Epoch 32: val_loss did not improve from 0.23100\n","146/146 [==============================] - 35s 239ms/step - loss: 0.2131 - accuracy: 0.7625 - val_loss: 0.2369 - val_accuracy: 0.7650\n","Epoch 33/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2087 - accuracy: 0.7766\n","Epoch 33: val_loss did not improve from 0.23100\n","146/146 [==============================] - 35s 239ms/step - loss: 0.2087 - accuracy: 0.7766 - val_loss: 0.2672 - val_accuracy: 0.7377\n","Epoch 34/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 0.7704\n","Epoch 34: val_loss did not improve from 0.23100\n","146/146 [==============================] - 36s 245ms/step - loss: 0.2148 - accuracy: 0.7704 - val_loss: 0.2498 - val_accuracy: 0.7486\n","Epoch 35/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2080 - accuracy: 0.7831\n","Epoch 35: val_loss did not improve from 0.23100\n","146/146 [==============================] - 36s 246ms/step - loss: 0.2080 - accuracy: 0.7831 - val_loss: 0.2629 - val_accuracy: 0.7186\n","Epoch 36/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2117 - accuracy: 0.7690\n","Epoch 36: val_loss did not improve from 0.23100\n","146/146 [==============================] - 35s 237ms/step - loss: 0.2117 - accuracy: 0.7690 - val_loss: 0.2511 - val_accuracy: 0.7486\n","Epoch 37/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.7766\n","Epoch 37: val_loss did not improve from 0.23100\n","146/146 [==============================] - 36s 248ms/step - loss: 0.2125 - accuracy: 0.7766 - val_loss: 0.2821 - val_accuracy: 0.7596\n","Epoch 38/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2135 - accuracy: 0.7618\n","Epoch 38: val_loss improved from 0.23100 to 0.22730, saving model to /content/drive/MyDrive/Diabetic/model.h5\n","146/146 [==============================] - 40s 272ms/step - loss: 0.2135 - accuracy: 0.7618 - val_loss: 0.2273 - val_accuracy: 0.7760\n","Epoch 39/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2067 - accuracy: 0.7769\n","Epoch 39: val_loss did not improve from 0.22730\n","146/146 [==============================] - 37s 254ms/step - loss: 0.2067 - accuracy: 0.7769 - val_loss: 0.2902 - val_accuracy: 0.7568\n","Epoch 40/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2081 - accuracy: 0.7745\n","Epoch 40: val_loss did not improve from 0.22730\n","146/146 [==============================] - 37s 251ms/step - loss: 0.2081 - accuracy: 0.7745 - val_loss: 0.2455 - val_accuracy: 0.7541\n","Epoch 41/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2058 - accuracy: 0.7704\n","Epoch 41: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 239ms/step - loss: 0.2058 - accuracy: 0.7704 - val_loss: 0.2599 - val_accuracy: 0.7158\n","Epoch 42/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2045 - accuracy: 0.7917\n","Epoch 42: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 248ms/step - loss: 0.2045 - accuracy: 0.7917 - val_loss: 0.2803 - val_accuracy: 0.7295\n","Epoch 43/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1976 - accuracy: 0.7858\n","Epoch 43: val_loss did not improve from 0.22730\n","146/146 [==============================] - 38s 259ms/step - loss: 0.1976 - accuracy: 0.7858 - val_loss: 0.2628 - val_accuracy: 0.7514\n","Epoch 44/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2068 - accuracy: 0.7796\n","Epoch 44: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 237ms/step - loss: 0.2068 - accuracy: 0.7796 - val_loss: 0.2685 - val_accuracy: 0.7541\n","Epoch 45/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1998 - accuracy: 0.7896\n","Epoch 45: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 249ms/step - loss: 0.1998 - accuracy: 0.7896 - val_loss: 0.2811 - val_accuracy: 0.7322\n","Epoch 46/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1969 - accuracy: 0.7862\n","Epoch 46: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 240ms/step - loss: 0.1969 - accuracy: 0.7862 - val_loss: 0.2549 - val_accuracy: 0.7787\n","Epoch 47/100\n","146/146 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.7817\n","Epoch 47: val_loss did not improve from 0.22730\n","146/146 [==============================] - 37s 252ms/step - loss: 0.2000 - accuracy: 0.7817 - val_loss: 0.2783 - val_accuracy: 0.7432\n","Epoch 48/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1962 - accuracy: 0.7910\n","Epoch 48: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 244ms/step - loss: 0.1962 - accuracy: 0.7910 - val_loss: 0.2876 - val_accuracy: 0.7623\n","Epoch 49/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.7893\n","Epoch 49: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 239ms/step - loss: 0.1973 - accuracy: 0.7893 - val_loss: 0.2499 - val_accuracy: 0.7514\n","Epoch 50/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.7937\n","Epoch 50: val_loss did not improve from 0.22730\n","146/146 [==============================] - 38s 259ms/step - loss: 0.1917 - accuracy: 0.7937 - val_loss: 0.2884 - val_accuracy: 0.7295\n","Epoch 51/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1978 - accuracy: 0.7869\n","Epoch 51: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 249ms/step - loss: 0.1978 - accuracy: 0.7869 - val_loss: 0.2824 - val_accuracy: 0.7568\n","Epoch 52/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1911 - accuracy: 0.7937\n","Epoch 52: val_loss did not improve from 0.22730\n","146/146 [==============================] - 38s 257ms/step - loss: 0.1911 - accuracy: 0.7937 - val_loss: 0.2922 - val_accuracy: 0.7650\n","Epoch 53/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1922 - accuracy: 0.7972\n","Epoch 53: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 239ms/step - loss: 0.1922 - accuracy: 0.7972 - val_loss: 0.2758 - val_accuracy: 0.7377\n","Epoch 54/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.7855\n","Epoch 54: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 239ms/step - loss: 0.1955 - accuracy: 0.7855 - val_loss: 0.2598 - val_accuracy: 0.7350\n","Epoch 55/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.7927\n","Epoch 55: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 242ms/step - loss: 0.1869 - accuracy: 0.7927 - val_loss: 0.3005 - val_accuracy: 0.7514\n","Epoch 56/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1863 - accuracy: 0.7965\n","Epoch 56: val_loss did not improve from 0.22730\n","146/146 [==============================] - 37s 251ms/step - loss: 0.1863 - accuracy: 0.7965 - val_loss: 0.3031 - val_accuracy: 0.7705\n","Epoch 57/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1882 - accuracy: 0.8044\n","Epoch 57: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 238ms/step - loss: 0.1882 - accuracy: 0.8044 - val_loss: 0.2814 - val_accuracy: 0.7596\n","Epoch 58/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.7948\n","Epoch 58: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 242ms/step - loss: 0.1912 - accuracy: 0.7948 - val_loss: 0.2832 - val_accuracy: 0.7350\n","Epoch 59/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1861 - accuracy: 0.7924\n","Epoch 59: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 238ms/step - loss: 0.1861 - accuracy: 0.7924 - val_loss: 0.2875 - val_accuracy: 0.7432\n","Epoch 60/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1899 - accuracy: 0.7900\n","Epoch 60: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 243ms/step - loss: 0.1899 - accuracy: 0.7900 - val_loss: 0.2872 - val_accuracy: 0.7568\n","Epoch 61/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1893 - accuracy: 0.7941\n","Epoch 61: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 249ms/step - loss: 0.1893 - accuracy: 0.7941 - val_loss: 0.2827 - val_accuracy: 0.7568\n","Epoch 62/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1814 - accuracy: 0.8078\n","Epoch 62: val_loss did not improve from 0.22730\n","146/146 [==============================] - 39s 264ms/step - loss: 0.1814 - accuracy: 0.8078 - val_loss: 0.3059 - val_accuracy: 0.7404\n","Epoch 63/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1850 - accuracy: 0.8030\n","Epoch 63: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 239ms/step - loss: 0.1850 - accuracy: 0.8030 - val_loss: 0.2870 - val_accuracy: 0.7295\n","Epoch 64/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.8054\n","Epoch 64: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 250ms/step - loss: 0.1821 - accuracy: 0.8054 - val_loss: 0.2892 - val_accuracy: 0.7678\n","Epoch 65/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1834 - accuracy: 0.8078\n","Epoch 65: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 240ms/step - loss: 0.1834 - accuracy: 0.8078 - val_loss: 0.3136 - val_accuracy: 0.7432\n","Epoch 66/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1832 - accuracy: 0.8017\n","Epoch 66: val_loss did not improve from 0.22730\n","146/146 [==============================] - 37s 250ms/step - loss: 0.1832 - accuracy: 0.8017 - val_loss: 0.2911 - val_accuracy: 0.7459\n","Epoch 67/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1841 - accuracy: 0.8020\n","Epoch 67: val_loss did not improve from 0.22730\n","146/146 [==============================] - 37s 252ms/step - loss: 0.1841 - accuracy: 0.8020 - val_loss: 0.2968 - val_accuracy: 0.7377\n","Epoch 68/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.8195\n","Epoch 68: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 247ms/step - loss: 0.1749 - accuracy: 0.8195 - val_loss: 0.2994 - val_accuracy: 0.7404\n","Epoch 69/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1764 - accuracy: 0.8092\n","Epoch 69: val_loss did not improve from 0.22730\n","146/146 [==============================] - 38s 259ms/step - loss: 0.1764 - accuracy: 0.8092 - val_loss: 0.2764 - val_accuracy: 0.7541\n","Epoch 70/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1766 - accuracy: 0.8068\n","Epoch 70: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 239ms/step - loss: 0.1766 - accuracy: 0.8068 - val_loss: 0.2982 - val_accuracy: 0.7268\n","Epoch 71/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1758 - accuracy: 0.8092\n","Epoch 71: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 248ms/step - loss: 0.1758 - accuracy: 0.8092 - val_loss: 0.2744 - val_accuracy: 0.7486\n","Epoch 72/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.8175\n","Epoch 72: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 244ms/step - loss: 0.1716 - accuracy: 0.8175 - val_loss: 0.3082 - val_accuracy: 0.7514\n","Epoch 73/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.8133\n","Epoch 73: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 245ms/step - loss: 0.1754 - accuracy: 0.8133 - val_loss: 0.3264 - val_accuracy: 0.7623\n","Epoch 74/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1736 - accuracy: 0.8212\n","Epoch 74: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 243ms/step - loss: 0.1736 - accuracy: 0.8212 - val_loss: 0.3306 - val_accuracy: 0.7268\n","Epoch 75/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.8195\n","Epoch 75: val_loss did not improve from 0.22730\n","146/146 [==============================] - 38s 259ms/step - loss: 0.1701 - accuracy: 0.8195 - val_loss: 0.3210 - val_accuracy: 0.7377\n","Epoch 76/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1766 - accuracy: 0.8175\n","Epoch 76: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 240ms/step - loss: 0.1766 - accuracy: 0.8175 - val_loss: 0.2883 - val_accuracy: 0.7568\n","Epoch 77/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1709 - accuracy: 0.8157\n","Epoch 77: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 241ms/step - loss: 0.1709 - accuracy: 0.8157 - val_loss: 0.2867 - val_accuracy: 0.7596\n","Epoch 78/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.8130\n","Epoch 78: val_loss did not improve from 0.22730\n","146/146 [==============================] - 37s 251ms/step - loss: 0.1717 - accuracy: 0.8130 - val_loss: 0.3340 - val_accuracy: 0.7596\n","Epoch 79/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.8206\n","Epoch 79: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 247ms/step - loss: 0.1716 - accuracy: 0.8206 - val_loss: 0.3054 - val_accuracy: 0.7377\n","Epoch 80/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1698 - accuracy: 0.8171\n","Epoch 80: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 244ms/step - loss: 0.1698 - accuracy: 0.8171 - val_loss: 0.3112 - val_accuracy: 0.7514\n","Epoch 81/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.8226\n","Epoch 81: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 243ms/step - loss: 0.1659 - accuracy: 0.8226 - val_loss: 0.3331 - val_accuracy: 0.7568\n","Epoch 82/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1640 - accuracy: 0.8250\n","Epoch 82: val_loss did not improve from 0.22730\n","146/146 [==============================] - 37s 250ms/step - loss: 0.1640 - accuracy: 0.8250 - val_loss: 0.3154 - val_accuracy: 0.7568\n","Epoch 83/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.8305\n","Epoch 83: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 240ms/step - loss: 0.1657 - accuracy: 0.8305 - val_loss: 0.3155 - val_accuracy: 0.7568\n","Epoch 84/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1649 - accuracy: 0.8230\n","Epoch 84: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 247ms/step - loss: 0.1649 - accuracy: 0.8230 - val_loss: 0.3084 - val_accuracy: 0.7514\n","Epoch 85/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1681 - accuracy: 0.8233\n","Epoch 85: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 239ms/step - loss: 0.1681 - accuracy: 0.8233 - val_loss: 0.3369 - val_accuracy: 0.7295\n","Epoch 86/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1670 - accuracy: 0.8199\n","Epoch 86: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 246ms/step - loss: 0.1670 - accuracy: 0.8199 - val_loss: 0.3235 - val_accuracy: 0.7350\n","Epoch 87/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.8212\n","Epoch 87: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 245ms/step - loss: 0.1711 - accuracy: 0.8212 - val_loss: 0.3074 - val_accuracy: 0.7486\n","Epoch 88/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.8278\n","Epoch 88: val_loss did not improve from 0.22730\n","146/146 [==============================] - 37s 250ms/step - loss: 0.1628 - accuracy: 0.8278 - val_loss: 0.3399 - val_accuracy: 0.7404\n","Epoch 89/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1639 - accuracy: 0.8316\n","Epoch 89: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 247ms/step - loss: 0.1639 - accuracy: 0.8316 - val_loss: 0.3246 - val_accuracy: 0.7322\n","Epoch 90/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1655 - accuracy: 0.8212\n","Epoch 90: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 240ms/step - loss: 0.1655 - accuracy: 0.8212 - val_loss: 0.3094 - val_accuracy: 0.7404\n","Epoch 91/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1637 - accuracy: 0.8326\n","Epoch 91: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 240ms/step - loss: 0.1637 - accuracy: 0.8326 - val_loss: 0.3493 - val_accuracy: 0.7623\n","Epoch 92/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.8267\n","Epoch 92: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 244ms/step - loss: 0.1616 - accuracy: 0.8267 - val_loss: 0.3151 - val_accuracy: 0.7432\n","Epoch 93/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1617 - accuracy: 0.8360\n","Epoch 93: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 244ms/step - loss: 0.1617 - accuracy: 0.8360 - val_loss: 0.3692 - val_accuracy: 0.7459\n","Epoch 94/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1593 - accuracy: 0.8316\n","Epoch 94: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 241ms/step - loss: 0.1593 - accuracy: 0.8316 - val_loss: 0.3491 - val_accuracy: 0.7404\n","Epoch 95/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.8377\n","Epoch 95: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 250ms/step - loss: 0.1540 - accuracy: 0.8377 - val_loss: 0.3098 - val_accuracy: 0.7568\n","Epoch 96/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1619 - accuracy: 0.8250\n","Epoch 96: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 240ms/step - loss: 0.1619 - accuracy: 0.8250 - val_loss: 0.2998 - val_accuracy: 0.7432\n","Epoch 97/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1563 - accuracy: 0.8412\n","Epoch 97: val_loss did not improve from 0.22730\n","146/146 [==============================] - 35s 239ms/step - loss: 0.1563 - accuracy: 0.8412 - val_loss: 0.3282 - val_accuracy: 0.7049\n","Epoch 98/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.8298\n","Epoch 98: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 243ms/step - loss: 0.1598 - accuracy: 0.8298 - val_loss: 0.3285 - val_accuracy: 0.7486\n","Epoch 99/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1546 - accuracy: 0.8429\n","Epoch 99: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 243ms/step - loss: 0.1546 - accuracy: 0.8429 - val_loss: 0.3867 - val_accuracy: 0.7268\n","Epoch 100/100\n","146/146 [==============================] - ETA: 0s - loss: 0.1592 - accuracy: 0.8333\n","Epoch 100: val_loss did not improve from 0.22730\n","146/146 [==============================] - 36s 249ms/step - loss: 0.1592 - accuracy: 0.8333 - val_loss: 0.3204 - val_accuracy: 0.7486\n"]}],"source":["history=model.fit(train_datagen.flow(X_train, y_train, batch_size = bs),\n","                            steps_per_epoch = len(X_train)//bs,\n","                            validation_data = (X_valid, y_valid),\n","                            validation_steps = len(X_valid)//bs,\n","                            epochs =epochs,\n","                            callbacks= [checkpointer])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2349,"status":"ok","timestamp":1677495561325,"user":{"displayName":"Ashish Kumbhar","userId":"04417448536066889477"},"user_tz":-330},"id":"dDgGavhEDBDP","outputId":"0da2c63c-55f9-49c1-f79c-27832227ed65"},"outputs":[{"output_type":"stream","name":"stdout","text":["19/19 - 2s - loss: 0.3405 - accuracy: 0.7357 - 2s/epoch - 86ms/step\n"]}],"source":["(eval_loss, eval_accuracy) = model.evaluate(  \n","     X_test, y_test, batch_size=bs, verbose=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1677495561326,"user":{"displayName":"Ashish Kumbhar","userId":"04417448536066889477"},"user_tz":-330},"id":"a8tqeYRWEaWp","outputId":"98227e7f-fe28-403e-a3ce-de78e5a10424"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 73.57%\n","Loss: 34.05%\n"]}],"source":["print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n","print(\"Loss: {:.2f}%\".format(eval_loss*100))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1r--aow4VY9ziKqSdm4r_SPa8Basjl3R-","authorship_tag":"ABX9TyNmHbcOB//IJ2Z+o3MYE6xn"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}